{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14457211,"sourceType":"datasetVersion","datasetId":9234156}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('rice disease detection')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===========================\n# 1. IMPORT LIBRARIES\n# ===========================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport cv2\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n\nprint(f\"TensorFlow Version: {tf.__version__}\")\nprint(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n\n# ===========================\n# 2. OPTIMIZED PARAMETERS\n# ===========================\nIMG_SIZE = 224  # Standard size for MobileNetV2\nBATCH_SIZE = 32  # Proper batch size\nEPOCHS = 50\nLEARNING_RATE = 0.001  # Standard starting LR\nSEED = 42\n\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\n# ===========================\n# 3. DATASET PATHS\n# ===========================\nBASE_DIR = '/kaggle/input/rice-dataset'\ntrain_dir = os.path.join(BASE_DIR, 'Rice_Leaf_Disease/Rice_Leaf_Diease/Rice_Leaf_Diease/train')\ntest_dir = os.path.join(BASE_DIR, 'Rice_Leaf_Disease/Rice_Leaf_Diease/Rice_Leaf_Diease/test')\n\nprint(\"=\"*70)\nprint(\"DATASET SETUP\")\nprint(\"=\"*70)\nprint(f\"Train: {train_dir}\")\nprint(f\"Test: {test_dir}\")\n\n# ===========================\n# 4. GET CLASS NAMES DIRECTLY FROM FOLDERS\n# ===========================\n# Don't hardcode - let Keras auto-detect!\ntrain_classes = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])\ntest_classes = sorted([d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))])\n\nprint(f\"\\nTrain classes ({len(train_classes)}):\")\nfor cls in train_classes:\n    count = len(os.listdir(os.path.join(train_dir, cls)))\n    print(f\"  {cls}: {count} images\")\n\nprint(f\"\\nTest classes ({len(test_classes)}):\")\nfor cls in test_classes:\n    count = len(os.listdir(os.path.join(test_dir, cls)))\n    print(f\"  {cls}: {count} images\")\n\n# Create mapping: normalize test names to match train names\ndef normalize_name(name):\n    return name.lower().replace(' ', '_')\n\n# Build train class mapping (normalized -> actual)\ntrain_norm_map = {normalize_name(c): c for c in train_classes}\ntest_norm_map = {normalize_name(c): c for c in test_classes}\n\n# Get consistent normalized class list (from train)\nCLASSES_NORMALIZED = sorted(train_norm_map.keys())\nNUM_CLASSES = len(CLASSES_NORMALIZED)\n\n# Map to actual folder names for generators\nTRAIN_CLASSES_ACTUAL = [train_norm_map[c] for c in CLASSES_NORMALIZED]\nTEST_CLASSES_ACTUAL = [test_norm_map.get(c, c) for c in CLASSES_NORMALIZED]\n\nprint(f\"\\nâœ… Using {NUM_CLASSES} classes in normalized order:\")\nfor i, (norm, train_actual, test_actual) in enumerate(zip(CLASSES_NORMALIZED, TRAIN_CLASSES_ACTUAL, TEST_CLASSES_ACTUAL)):\n    print(f\"  {i}: {norm:30s} | Train: {train_actual:30s} | Test: {test_actual:30s}\")\n\n# ===========================\n# 5. DATA GENERATORS WITH CORRECT PREPROCESSING\n# ===========================\nprint(\"\\n\" + \"=\"*70)\nprint(\"CREATING DATA GENERATORS\")\nprint(\"=\"*70)\n\n# CRITICAL: Use preprocessing_function for MobileNetV2\n# This applies the correct normalization (-1 to 1 range)\ntrain_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # MobileNetV2 preprocessing!\n    rotation_range=20,  # Moderate augmentation\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True,\n    vertical_flip=True,\n    fill_mode='nearest',\n    validation_split=0.2\n)\n\ntest_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input  # Same preprocessing!\n)\n\n# Create generators\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    classes=TRAIN_CLASSES_ACTUAL,\n    subset='training',\n    shuffle=True,\n    seed=SEED\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    classes=TRAIN_CLASSES_ACTUAL,\n    subset='validation',\n    shuffle=False,\n    seed=SEED\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    classes=TEST_CLASSES_ACTUAL,\n    shuffle=False\n)\n\nprint(f\"\\nâœ… Train samples: {train_generator.samples}\")\nprint(f\"âœ… Validation samples: {validation_generator.samples}\")\nprint(f\"âœ… Test samples: {test_generator.samples}\")\nprint(f\"âœ… Steps per epoch: {len(train_generator)}\")\n\n# Verify class indices\nprint(f\"\\nğŸ“Š Class indices verification:\")\nprint(f\"Train: {train_generator.class_indices}\")\nprint(f\"Test: {test_generator.class_indices}\")\n\n# ===========================\n# 6. BUILD MODEL - MobileNetV2 (Lightweight & Fast)\n# ===========================\nprint(\"\\n\" + \"=\"*70)\nprint(\"BUILDING MODEL - MobileNetV2\")\nprint(\"=\"*70)\n\ndef build_mobilenet_model(num_classes, img_size=224):\n    \"\"\"Build MobileNetV2-based model\"\"\"\n    \n    inputs = layers.Input(shape=(img_size, img_size, 3))\n    \n    # Load MobileNetV2 (much lighter than EfficientNetB3)\n    base_model = MobileNetV2(\n        include_top=False,\n        weights='imagenet',\n        input_tensor=inputs,\n        pooling='avg'\n    )\n    \n    # Start with frozen base\n    base_model.trainable = False\n    \n    # Simple classification head\n    x = base_model.output\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.2)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    \n    model = models.Model(inputs=inputs, outputs=outputs)\n    \n    return model, base_model\n\nmodel, base_model = build_mobilenet_model(NUM_CLASSES, IMG_SIZE)\n\nprint(f\"âœ… Total parameters: {model.count_params():,}\")\nprint(f\"âœ… Trainable parameters: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\")\n\n# ===========================\n# 7. COMPILE MODEL\n# ===========================\nmodel.compile(\n    optimizer=Adam(learning_rate=LEARNING_RATE),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# ===========================\n# 8. CALLBACKS\n# ===========================\ncallbacks = [\n    EarlyStopping(\n        monitor='val_accuracy',\n        patience=10,\n        restore_best_weights=True,\n        mode='max',\n        verbose=1\n    ),\n    ReduceLROnPlateau(\n        monitor='val_accuracy',\n        factor=0.5,\n        patience=5,\n        mode='max',\n        min_lr=1e-7,\n        verbose=1\n    ),\n    ModelCheckpoint(\n        'best_rice_mobilenet.h5',\n        monitor='val_accuracy',\n        save_best_only=True,\n        mode='max',\n        verbose=1\n    )\n]\n\n# ===========================\n# 9. TRAIN - PHASE 1 (Frozen Base)\n# ===========================\nprint(\"\\n\" + \"=\"*70)\nprint(\"PHASE 1: Training with Frozen Base (15 epochs)\")\nprint(\"=\"*70)\n\nhistory1 = model.fit(\n    train_generator,\n    validation_data=validation_generator,\n    epochs=15,\n    callbacks=callbacks,\n    verbose=1\n)\n\nprint(f\"\\nğŸ“Š Phase 1 Best Val Accuracy: {max(history1.history['val_accuracy']):.4f}\")\n\n# ===========================\n# 10. TRAIN - PHASE 2 (Fine-tuning)\n# ===========================\nprint(\"\\n\" + \"=\"*70)\nprint(\"PHASE 2: Fine-tuning (Unfreeze last layers)\")\nprint(\"=\"*70)\n\n# Unfreeze last 30 layers\nbase_model.trainable = True\nfor layer in base_model.layers[:-30]:\n    layer.trainable = False\n\nprint(f\"Trainable layers: {sum([layer.trainable for layer in model.layers])}\")\n\n# Recompile with lower LR\nmodel.compile(\n    optimizer=Adam(learning_rate=LEARNING_RATE/10),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory2 = model.fit(\n    train_generator,\n    validation_data=validation_generator,\n    epochs=EPOCHS,\n    initial_epoch=len(history1.history['loss']),\n    callbacks=callbacks,\n    verbose=1\n)\n\n# ===========================\n# 11. COMBINE & PLOT HISTORY\n# ===========================\nhistory_combined = {\n    'accuracy': history1.history['accuracy'] + history2.history['accuracy'],\n    'val_accuracy': history1.history['val_accuracy'] + history2.history['val_accuracy'],\n    'loss': history1.history['loss'] + history2.history['loss'],\n    'val_loss': history1.history['val_loss'] + history2.history['val_loss']\n}\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Accuracy\naxes[0].plot(history_combined['accuracy'], label='Train', linewidth=2)\naxes[0].plot(history_combined['val_accuracy'], label='Validation', linewidth=2)\naxes[0].axvline(x=15, color='red', linestyle='--', label='Fine-tuning starts')\naxes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Accuracy')\naxes[0].legend()\naxes[0].grid(alpha=0.3)\n\n# Loss\naxes[1].plot(history_combined['loss'], label='Train', linewidth=2)\naxes[1].plot(history_combined['val_loss'], label='Validation', linewidth=2)\naxes[1].axvline(x=15, color='red', linestyle='--', label='Fine-tuning starts')\naxes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Loss')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nbest_val_acc = max(history_combined['val_accuracy'])\nbest_epoch = history_combined['val_accuracy'].index(best_val_acc) + 1\nprint(f\"\\nğŸ“Š Best Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%) at epoch {best_epoch}\")\n\n# ===========================\n# 12. LOAD BEST MODEL & EVALUATE\n# ===========================\nprint(\"\\n\" + \"=\"*70)\nprint(\"LOADING BEST MODEL & EVALUATING ON TEST SET\")\nprint(\"=\"*70)\n\nmodel = keras.models.load_model('best_rice_mobilenet.h5')\n\ntest_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n\nprint(f\"\\n{'='*70}\")\nprint(\"TEST RESULTS\")\nprint(f\"{'='*70}\")\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n\n# ===========================\n# 13. PREDICTIONS & CLASSIFICATION REPORT\n# ===========================\ntest_generator.reset()\ny_pred_probs = model.predict(test_generator, verbose=1)\ny_pred = np.argmax(y_pred_probs, axis=1)\ny_true = test_generator.classes\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"CLASSIFICATION REPORT\")\nprint(\"=\"*70)\nprint(classification_report(y_true, y_pred, target_names=CLASSES_NORMALIZED, digits=4))\n\n# ===========================\n# 14. CONFUSION MATRICES\n# ===========================\ncm = confusion_matrix(y_true, y_pred)\n\nfig, axes = plt.subplots(1, 2, figsize=(22, 9))\n\n# Raw counts\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=CLASSES_NORMALIZED,\n            yticklabels=CLASSES_NORMALIZED,\n            ax=axes[0])\naxes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\naxes[0].set_ylabel('True Label')\naxes[0].set_xlabel('Predicted Label')\n\n# Normalized\ncm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Greens',\n            xticklabels=CLASSES_NORMALIZED,\n            yticklabels=CLASSES_NORMALIZED,\n            ax=axes[1])\naxes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\naxes[1].set_ylabel('True Label')\naxes[1].set_xlabel('Predicted Label')\n\nplt.tight_layout()\nplt.show()\n\n# ===========================\n# 15. VISUALIZE PREDICTIONS\n# ===========================\ndef visualize_predictions(n=16):\n    test_generator.reset()\n    images_raw, labels = next(test_generator)\n    \n    # Reverse preprocessing to display properly\n    images_display = (images_raw + 1.0) / 2.0  # MobileNetV2 uses [-1, 1] range\n    \n    preds = model.predict(images_raw[:n], verbose=0)\n    \n    fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n    axes = axes.flatten()\n    \n    for i in range(n):\n        axes[i].imshow(images_display[i])\n        \n        true_idx = np.argmax(labels[i])\n        pred_idx = np.argmax(preds[i])\n        true_label = CLASSES_NORMALIZED[true_idx]\n        pred_label = CLASSES_NORMALIZED[pred_idx]\n        conf = preds[i][pred_idx] * 100\n        \n        color = 'green' if true_idx == pred_idx else 'red'\n        axes[i].set_title(f'True: {true_label}\\nPred: {pred_label}\\n{conf:.1f}%',\n                         fontsize=9, color=color, fontweight='bold')\n        axes[i].axis('off')\n    \n    plt.suptitle('Model Predictions', fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\nvisualize_predictions(16)\n\n# ===========================\n# 16. PER-CLASS METRICS\n# ===========================\nprecision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)\n\ndf = pd.DataFrame({\n    'Class': CLASSES_NORMALIZED,\n    'Precision': precision,\n    'Recall': recall,\n    'F1-Score': f1,\n    'Support': support\n})\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"PER-CLASS PERFORMANCE\")\nprint(\"=\"*70)\nprint(df.to_string(index=False))\n\n# Plot\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\naxes[0].barh(CLASSES_NORMALIZED, precision, color='steelblue')\naxes[0].set_xlabel('Precision')\naxes[0].set_title('Precision by Class', fontweight='bold')\naxes[0].set_xlim([0, 1])\n\naxes[1].barh(CLASSES_NORMALIZED, recall, color='coral')\naxes[1].set_xlabel('Recall')\naxes[1].set_title('Recall by Class', fontweight='bold')\naxes[1].set_xlim([0, 1])\n\naxes[2].barh(CLASSES_NORMALIZED, f1, color='mediumseagreen')\naxes[2].set_xlabel('F1-Score')\naxes[2].set_title('F1-Score by Class', fontweight='bold')\naxes[2].set_xlim([0, 1])\n\nplt.tight_layout()\nplt.show()\n\n# ===========================\n# 17. SAVE MODEL\n# ===========================\nmodel.save('rice_disease_mobilenet_final.h5')\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… TRAINING COMPLETE!\")\nprint(\"=\"*70)\nprint(f\"âœ… Best model: best_rice_mobilenet.h5\")\nprint(f\"âœ… Final model: rice_disease_mobilenet_final.h5\")\nprint(f\"âœ… Test Accuracy: {test_accuracy*100:.2f}%\")\nprint(\"=\"*70)\n\n# ===========================\n# 18. PREDICTION FUNCTION\n# ===========================\ndef predict_image(image_path):\n    \"\"\"Predict disease for a single image\"\"\"\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img_resized = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n    \n    # Apply same preprocessing as training\n    img_preprocessed = preprocess_input(img_resized)\n    img_batch = np.expand_dims(img_preprocessed, axis=0)\n    \n    # Predict\n    pred = model.predict(img_batch, verbose=0)[0]\n    pred_idx = np.argmax(pred)\n    pred_class = CLASSES_NORMALIZED[pred_idx]\n    confidence = pred[pred_idx] * 100\n    \n    # Get top 3\n    top3_idx = np.argsort(pred)[-3:][::-1]\n    \n    # Display\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    \n    axes[0].imshow(img)\n    axes[0].set_title('Input Image', fontweight='bold')\n    axes[0].axis('off')\n    \n    top3_classes = [CLASSES_NORMALIZED[i] for i in top3_idx]\n    top3_probs = [pred[i]*100 for i in top3_idx]\n    axes[1].barh(top3_classes, top3_probs, color=['green', 'steelblue', 'coral'])\n    axes[1].set_xlabel('Confidence (%)')\n    axes[1].set_title('Top 3 Predictions', fontweight='bold')\n    axes[1].set_xlim([0, 100])\n    \n    plt.suptitle(f'Prediction: {pred_class} ({confidence:.2f}%)', \n                fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    return pred_class, confidence\n\nprint(\"\\nâœ… Prediction function ready!\")\nprint(\"Usage: predict_image('/path/to/image.jpg')\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T11:43:13.667648Z","iopub.execute_input":"2026-01-12T11:43:13.668339Z","execution_failed":"2026-01-12T15:21:41.115Z"}},"outputs":[{"name":"stderr","text":"2026-01-12 11:43:19.080460: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768218199.511091      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768218199.640439      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768218200.612190      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768218200.612231      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768218200.612234      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768218200.612237      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"TensorFlow Version: 2.19.0\nGPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n======================================================================\nDATASET SETUP\n======================================================================\nTrain: /kaggle/input/rice-dataset/Rice_Leaf_Disease/Rice_Leaf_Diease/Rice_Leaf_Diease/train\nTest: /kaggle/input/rice-dataset/Rice_Leaf_Disease/Rice_Leaf_Diease/Rice_Leaf_Diease/test\n\nTrain classes (10):\n  bacterial_leaf_blight: 1386 images\n  brown_spot: 1480 images\n  healthy: 1491 images\n  leaf_blast: 1801 images\n  leaf_scald: 1670 images\n  narrow_brown_spot: 1416 images\n  neck_blast: 1000 images\n  rice_hispa: 1461 images\n  sheath_blight: 1578 images\n  tungro: 1740 images\n\nTest classes (10):\n  Neck_Blast: 322 images\n  Rice Hispa: 225 images\n  Sheath Blight: 288 images\n  Tungro: 310 images\n  bacterial_leaf_blight: 376 images\n  brown_spot: 380 images\n  healthy: 391 images\n  leaf_blast: 362 images\n  leaf_scald: 386 images\n  narrow_brown_spot: 382 images\n\nâœ… Using 10 classes in normalized order:\n  0: bacterial_leaf_blight          | Train: bacterial_leaf_blight          | Test: bacterial_leaf_blight         \n  1: brown_spot                     | Train: brown_spot                     | Test: brown_spot                    \n  2: healthy                        | Train: healthy                        | Test: healthy                       \n  3: leaf_blast                     | Train: leaf_blast                     | Test: leaf_blast                    \n  4: leaf_scald                     | Train: leaf_scald                     | Test: leaf_scald                    \n  5: narrow_brown_spot              | Train: narrow_brown_spot              | Test: narrow_brown_spot             \n  6: neck_blast                     | Train: neck_blast                     | Test: Neck_Blast                    \n  7: rice_hispa                     | Train: rice_hispa                     | Test: Rice Hispa                    \n  8: sheath_blight                  | Train: sheath_blight                  | Test: Sheath Blight                 \n  9: tungro                         | Train: tungro                         | Test: Tungro                        \n\n======================================================================\nCREATING DATA GENERATORS\n======================================================================\nFound 12020 images belonging to 10 classes.\nFound 3003 images belonging to 10 classes.\nFound 3422 images belonging to 10 classes.\n\nâœ… Train samples: 12020\nâœ… Validation samples: 3003\nâœ… Test samples: 3422\nâœ… Steps per epoch: 376\n\nğŸ“Š Class indices verification:\nTrain: {'bacterial_leaf_blight': 0, 'brown_spot': 1, 'healthy': 2, 'leaf_blast': 3, 'leaf_scald': 4, 'narrow_brown_spot': 5, 'neck_blast': 6, 'rice_hispa': 7, 'sheath_blight': 8, 'tungro': 9}\nTest: {'bacterial_leaf_blight': 0, 'brown_spot': 1, 'healthy': 2, 'leaf_blast': 3, 'leaf_scald': 4, 'narrow_brown_spot': 5, 'Neck_Blast': 6, 'Rice Hispa': 7, 'Sheath Blight': 8, 'Tungro': 9}\n\n======================================================================\nBUILDING MODEL - MobileNetV2\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1768218230.787510      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1768218230.791399      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n\u001b[1m9406464/9406464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nâœ… Total parameters: 2,423,242\nâœ… Trainable parameters: 165,258\n\n======================================================================\nPHASE 1: Training with Frozen Base (15 epochs)\n======================================================================\nEpoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1768218241.693909     153 service.cc:152] XLA service 0x79b8881437a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1768218241.693955     153 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1768218241.693960     153 service.cc:160]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1768218243.058819     153 cuda_dnn.cc:529] Loaded cuDNN version 91002\n2026-01-12 11:44:12.352045: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2026-01-12 11:44:12.488782: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nI0000 00:00:1768218254.989288     153 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m330/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m42s\u001b[0m 919ms/step - accuracy: 0.5753 - loss: 1.2438","output_type":"stream"},{"name":"stderr","text":"2026-01-12 11:49:26.223726: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2026-01-12 11:49:26.359351: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939ms/step - accuracy: 0.5900 - loss: 1.1997","output_type":"stream"},{"name":"stderr","text":"2026-01-12 11:51:44.090159: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2026-01-12 11:51:44.227252: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: val_accuracy improved from -inf to 0.85015, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m471s\u001b[0m 1s/step - accuracy: 0.5903 - loss: 1.1989 - val_accuracy: 0.8501 - val_loss: 0.4535 - learning_rate: 0.0010\nEpoch 2/15\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666ms/step - accuracy: 0.8036 - loss: 0.5689\nEpoch 2: val_accuracy improved from 0.85015 to 0.87379, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 839ms/step - accuracy: 0.8036 - loss: 0.5689 - val_accuracy: 0.8738 - val_loss: 0.3673 - learning_rate: 0.0010\nEpoch 3/15\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662ms/step - accuracy: 0.8296 - loss: 0.4876\nEpoch 3: val_accuracy improved from 0.87379 to 0.87546, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 825ms/step - accuracy: 0.8296 - loss: 0.4876 - val_accuracy: 0.8755 - val_loss: 0.3388 - learning_rate: 0.0010\nEpoch 4/15\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672ms/step - accuracy: 0.8541 - loss: 0.4139\nEpoch 4: val_accuracy improved from 0.87546 to 0.88145, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 841ms/step - accuracy: 0.8541 - loss: 0.4139 - val_accuracy: 0.8815 - val_loss: 0.3177 - learning_rate: 0.0010\nEpoch 5/15\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670ms/step - accuracy: 0.8573 - loss: 0.4032\nEpoch 5: val_accuracy improved from 0.88145 to 0.90509, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 841ms/step - accuracy: 0.8573 - loss: 0.4032 - val_accuracy: 0.9051 - val_loss: 0.2674 - learning_rate: 0.0010\nEpoch 6/15\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673ms/step - accuracy: 0.8651 - loss: 0.3706\nEpoch 6: val_accuracy improved from 0.90509 to 0.91042, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 837ms/step - accuracy: 0.8651 - loss: 0.3706 - val_accuracy: 0.9104 - val_loss: 0.2536 - learning_rate: 0.0010\nEpoch 7/15\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665ms/step - accuracy: 0.8700 - loss: 0.3622\nEpoch 7: val_accuracy improved from 0.91042 to 0.91875, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 834ms/step - accuracy: 0.8700 - loss: 0.3622 - val_accuracy: 0.9187 - val_loss: 0.2396 - learning_rate: 0.0010\nEpoch 8/15\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678ms/step - accuracy: 0.8804 - loss: 0.3332\nEpoch 8: val_accuracy did not improve from 0.91875\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 849ms/step - accuracy: 0.8804 - loss: 0.3332 - val_accuracy: 0.9094 - val_loss: 0.2419 - learning_rate: 0.0010\nEpoch 9/15\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664ms/step - accuracy: 0.8808 - loss: 0.3291\nEpoch 9: val_accuracy improved from 0.91875 to 0.92008, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 830ms/step - accuracy: 0.8808 - loss: 0.3291 - val_accuracy: 0.9201 - val_loss: 0.2321 - learning_rate: 0.0010\nEpoch 10/15\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669ms/step - accuracy: 0.8848 - loss: 0.3158\nEpoch 10: val_accuracy improved from 0.92008 to 0.92208, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 839ms/step - accuracy: 0.8848 - loss: 0.3158 - val_accuracy: 0.9221 - val_loss: 0.2192 - learning_rate: 0.0010\nEpoch 11/15\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674ms/step - accuracy: 0.8882 - loss: 0.3108\nEpoch 11: val_accuracy improved from 0.92208 to 0.92541, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 841ms/step - accuracy: 0.8882 - loss: 0.3108 - val_accuracy: 0.9254 - val_loss: 0.2042 - learning_rate: 0.0010\nEpoch 12/15\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676ms/step - accuracy: 0.8920 - loss: 0.2960\nEpoch 12: val_accuracy improved from 0.92541 to 0.92707, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 848ms/step - accuracy: 0.8920 - loss: 0.2960 - val_accuracy: 0.9271 - val_loss: 0.2087 - learning_rate: 0.0010\nEpoch 13/15\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662ms/step - accuracy: 0.8917 - loss: 0.3053\nEpoch 13: val_accuracy improved from 0.92707 to 0.93273, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 829ms/step - accuracy: 0.8917 - loss: 0.3053 - val_accuracy: 0.9327 - val_loss: 0.1921 - learning_rate: 0.0010\nEpoch 14/15\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666ms/step - accuracy: 0.8952 - loss: 0.2852\nEpoch 14: val_accuracy did not improve from 0.93273\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 835ms/step - accuracy: 0.8952 - loss: 0.2852 - val_accuracy: 0.9224 - val_loss: 0.2048 - learning_rate: 0.0010\nEpoch 15/15\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677ms/step - accuracy: 0.9023 - loss: 0.2664\nEpoch 15: val_accuracy did not improve from 0.93273\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 845ms/step - accuracy: 0.9023 - loss: 0.2664 - val_accuracy: 0.9184 - val_loss: 0.2133 - learning_rate: 0.0010\nRestoring model weights from the end of the best epoch: 13.\n\nğŸ“Š Phase 1 Best Val Accuracy: 0.9327\n\n======================================================================\nPHASE 2: Fine-tuning (Unfreeze last layers)\n======================================================================\nTrainable layers: 34\nEpoch 16/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693ms/step - accuracy: 0.7981 - loss: 0.6367\nEpoch 16: val_accuracy did not improve from 0.93273\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 878ms/step - accuracy: 0.7982 - loss: 0.6362 - val_accuracy: 0.9154 - val_loss: 0.2505 - learning_rate: 1.0000e-04\nEpoch 17/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676ms/step - accuracy: 0.9083 - loss: 0.2674\nEpoch 17: val_accuracy did not improve from 0.93273\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 840ms/step - accuracy: 0.9083 - loss: 0.2674 - val_accuracy: 0.9177 - val_loss: 0.2455 - learning_rate: 1.0000e-04\nEpoch 18/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665ms/step - accuracy: 0.9254 - loss: 0.2105\nEpoch 18: val_accuracy improved from 0.93273 to 0.93407, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 833ms/step - accuracy: 0.9254 - loss: 0.2104 - val_accuracy: 0.9341 - val_loss: 0.1803 - learning_rate: 1.0000e-04\nEpoch 19/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670ms/step - accuracy: 0.9377 - loss: 0.1660\nEpoch 19: val_accuracy improved from 0.93407 to 0.95171, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 837ms/step - accuracy: 0.9377 - loss: 0.1660 - val_accuracy: 0.9517 - val_loss: 0.1398 - learning_rate: 1.0000e-04\nEpoch 20/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676ms/step - accuracy: 0.9486 - loss: 0.1484\nEpoch 20: val_accuracy did not improve from 0.95171\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 847ms/step - accuracy: 0.9486 - loss: 0.1484 - val_accuracy: 0.9510 - val_loss: 0.1372 - learning_rate: 1.0000e-04\nEpoch 21/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666ms/step - accuracy: 0.9573 - loss: 0.1216\nEpoch 21: val_accuracy improved from 0.95171 to 0.96270, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 838ms/step - accuracy: 0.9573 - loss: 0.1216 - val_accuracy: 0.9627 - val_loss: 0.1028 - learning_rate: 1.0000e-04\nEpoch 22/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661ms/step - accuracy: 0.9580 - loss: 0.1146\nEpoch 22: val_accuracy improved from 0.96270 to 0.97036, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 832ms/step - accuracy: 0.9581 - loss: 0.1146 - val_accuracy: 0.9704 - val_loss: 0.0854 - learning_rate: 1.0000e-04\nEpoch 23/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686ms/step - accuracy: 0.9647 - loss: 0.1023\nEpoch 23: val_accuracy did not improve from 0.97036\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 859ms/step - accuracy: 0.9647 - loss: 0.1023 - val_accuracy: 0.9694 - val_loss: 0.0830 - learning_rate: 1.0000e-04\nEpoch 24/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662ms/step - accuracy: 0.9716 - loss: 0.0867\nEpoch 24: val_accuracy did not improve from 0.97036\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 824ms/step - accuracy: 0.9716 - loss: 0.0867 - val_accuracy: 0.9670 - val_loss: 0.0912 - learning_rate: 1.0000e-04\nEpoch 25/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666ms/step - accuracy: 0.9688 - loss: 0.0918\nEpoch 25: val_accuracy improved from 0.97036 to 0.97769, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 831ms/step - accuracy: 0.9688 - loss: 0.0918 - val_accuracy: 0.9777 - val_loss: 0.0683 - learning_rate: 1.0000e-04\nEpoch 26/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660ms/step - accuracy: 0.9711 - loss: 0.0827\nEpoch 26: val_accuracy did not improve from 0.97769\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 827ms/step - accuracy: 0.9711 - loss: 0.0827 - val_accuracy: 0.9767 - val_loss: 0.0737 - learning_rate: 1.0000e-04\nEpoch 27/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670ms/step - accuracy: 0.9766 - loss: 0.0663\nEpoch 27: val_accuracy did not improve from 0.97769\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 836ms/step - accuracy: 0.9766 - loss: 0.0663 - val_accuracy: 0.9740 - val_loss: 0.0891 - learning_rate: 1.0000e-04\nEpoch 28/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674ms/step - accuracy: 0.9768 - loss: 0.0707\nEpoch 28: val_accuracy did not improve from 0.97769\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 840ms/step - accuracy: 0.9768 - loss: 0.0707 - val_accuracy: 0.9660 - val_loss: 0.0997 - learning_rate: 1.0000e-04\nEpoch 29/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675ms/step - accuracy: 0.9793 - loss: 0.0628\nEpoch 29: val_accuracy did not improve from 0.97769\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 847ms/step - accuracy: 0.9793 - loss: 0.0628 - val_accuracy: 0.9750 - val_loss: 0.0690 - learning_rate: 1.0000e-04\nEpoch 30/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688ms/step - accuracy: 0.9779 - loss: 0.0613\nEpoch 30: val_accuracy improved from 0.97769 to 0.98135, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 856ms/step - accuracy: 0.9779 - loss: 0.0613 - val_accuracy: 0.9814 - val_loss: 0.0508 - learning_rate: 1.0000e-04\nEpoch 31/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683ms/step - accuracy: 0.9794 - loss: 0.0601\nEpoch 31: val_accuracy improved from 0.98135 to 0.98335, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 857ms/step - accuracy: 0.9794 - loss: 0.0601 - val_accuracy: 0.9833 - val_loss: 0.0572 - learning_rate: 1.0000e-04\nEpoch 32/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681ms/step - accuracy: 0.9816 - loss: 0.0546\nEpoch 32: val_accuracy did not improve from 0.98335\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 845ms/step - accuracy: 0.9816 - loss: 0.0546 - val_accuracy: 0.9817 - val_loss: 0.0485 - learning_rate: 1.0000e-04\nEpoch 33/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665ms/step - accuracy: 0.9857 - loss: 0.0454\nEpoch 33: val_accuracy improved from 0.98335 to 0.98435, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 841ms/step - accuracy: 0.9857 - loss: 0.0454 - val_accuracy: 0.9843 - val_loss: 0.0431 - learning_rate: 1.0000e-04\nEpoch 34/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676ms/step - accuracy: 0.9839 - loss: 0.0443\nEpoch 34: val_accuracy improved from 0.98435 to 0.98468, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 849ms/step - accuracy: 0.9839 - loss: 0.0443 - val_accuracy: 0.9847 - val_loss: 0.0407 - learning_rate: 1.0000e-04\nEpoch 35/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666ms/step - accuracy: 0.9793 - loss: 0.0551\nEpoch 35: val_accuracy improved from 0.98468 to 0.99068, saving model to best_rice_mobilenet.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 832ms/step - accuracy: 0.9793 - loss: 0.0551 - val_accuracy: 0.9907 - val_loss: 0.0245 - learning_rate: 1.0000e-04\nEpoch 36/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679ms/step - accuracy: 0.9840 - loss: 0.0454\nEpoch 36: val_accuracy did not improve from 0.99068\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 849ms/step - accuracy: 0.9840 - loss: 0.0454 - val_accuracy: 0.9830 - val_loss: 0.0514 - learning_rate: 1.0000e-04\nEpoch 37/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684ms/step - accuracy: 0.9837 - loss: 0.0451\nEpoch 37: val_accuracy did not improve from 0.99068\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 859ms/step - accuracy: 0.9837 - loss: 0.0451 - val_accuracy: 0.9820 - val_loss: 0.0472 - learning_rate: 1.0000e-04\nEpoch 38/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693ms/step - accuracy: 0.9847 - loss: 0.0450\nEpoch 38: val_accuracy did not improve from 0.99068\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 876ms/step - accuracy: 0.9847 - loss: 0.0450 - val_accuracy: 0.9843 - val_loss: 0.0352 - learning_rate: 1.0000e-04\nEpoch 39/50\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690ms/step - accuracy: 0.9862 - loss: 0.0390\nEpoch 39: val_accuracy did not improve from 0.99068\n\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 865ms/step - accuracy: 0.9862 - loss: 0.0390 - val_accuracy: 0.9883 - val_loss: 0.0324 - learning_rate: 1.0000e-04\nEpoch 40/50\n\u001b[1m  1/376\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m4:46\u001b[0m 764ms/step - accuracy: 0.9688 - loss: 0.0911","output_type":"stream"}],"execution_count":null}]}